{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAQ70Fi5949jnThfUTSePT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricSiq/DeepLearning/blob/main/DLLIrisFeedforwardNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k3s6okjpXqMA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FeedForwardNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.weights1 = np.random.randn(input_size, hidden_size) * 0.01 # Smaller initial weights\n",
        "        self.bias1 = np.zeros((1, hidden_size))\n",
        "        self.weights2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.bias2 = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x) # Derivative of sigmoid, assuming x is already sigmoid(activation)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True)) # For numerical stability\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Hidden layer\n",
        "        self.hidden_layer_activation = np.dot(x, self.weights1) + self.bias1\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_activation)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer_activation = np.dot(self.hidden_layer_output, self.weights2) + self.bias2\n",
        "        self.predictions = self.softmax(self.output_layer_activation) # Using softmax for multi-class\n",
        "\n",
        "        return self.predictions\n",
        "\n",
        "    def backward(self, X, Y, learning_rate):\n",
        "        num_samples = X.shape[0]\n",
        "\n",
        "        # Calculate gradients for output layer\n",
        "        # For cross-entropy with softmax, the error is simply (predictions - Y_true)\n",
        "        error_output = self.predictions - Y\n",
        "        d_weights2 = np.dot(self.hidden_layer_output.T, error_output) / num_samples\n",
        "        d_bias2 = np.sum(error_output, axis=0, keepdims=True) / num_samples\n",
        "\n",
        "        # Calculate gradients for hidden layer\n",
        "        error_hidden = np.dot(error_output, self.weights2.T) * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "        d_weights1 = np.dot(X.T, error_hidden) / num_samples\n",
        "        d_bias1 = np.sum(error_hidden, axis=0, keepdims=True) / num_samples\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights2 -= learning_rate * d_weights2\n",
        "        self.bias2 -= learning_rate * d_bias2\n",
        "        self.weights1 -= learning_rate * d_weights1\n",
        "        self.bias1 -= learning_rate * d_bias1"
      ],
      "metadata": {
        "id": "w8T4S-GPXtb_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F1MqUKeHX6zL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Loss functions\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    # Avoid log(0) by clipping predictions\n",
        "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "# Accuracy metric\n",
        "def accuracy(y_true, y_pred):\n",
        "    # Convert one-hot encoded to class labels\n",
        "    y_true_labels = np.argmax(y_true, axis=1)\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "    return np.mean(y_true_labels == y_pred_labels)"
      ],
      "metadata": {
        "id": "nfA9Nz8BX9fG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# One-hot encode the target labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "Y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features (important for NN performance)\n",
        "X_mean = X_train.mean(axis=0)\n",
        "X_std = X_train.std(axis=0)\n",
        "X_train = (X_train - X_mean) / (X_std + 1e-8) # Add small epsilon to avoid division by zero\n",
        "X_test = (X_test - X_mean) / (X_std + 1e-8)\n",
        "\n",
        "# Initialize the neural network\n",
        "input_size = X_train.shape[1]  # 4 features\n",
        "hidden_size = 10               # Can be tuned\n",
        "output_size = Y_train.shape[1] # 3 classes"
      ],
      "metadata": {
        "id": "A8DByeHtYBw8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = FeedForwardNN(input_size, hidden_size, output_size)\n",
        "\n",
        "print(\"--- Before Training ---\")\n",
        "predictions_before = model.forward(X_test)\n",
        "loss_before = cross_entropy_loss(Y_test, predictions_before)\n",
        "accuracy_before = accuracy(Y_test, predictions_before)\n",
        "print(f\"Loss: {loss_before:.4f}\")\n",
        "print(f\"Accuracy: {accuracy_before:.4f}\")\n",
        "\n",
        "# Training loop\n",
        "learning_rate = 0.01\n",
        "epochs = 5000  # Increased epochs for better training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0WVDsKvYEJL",
        "outputId": "178e3b1f-e679-48de-ae22-a41acf2df1e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Before Training ---\n",
            "Loss: 1.0982\n",
            "Accuracy: 0.3667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Training ---\")\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    predictions_train = model.forward(X_train)\n",
        "\n",
        "    # Backward pass (update weights and biases)\n",
        "    model.backward(X_train, Y_train, learning_rate)\n",
        "\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        loss_train = cross_entropy_loss(Y_train, predictions_train)\n",
        "        accuracy_train = accuracy(Y_train, predictions_train)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss_train:.4f}, Accuracy: {accuracy_train:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJb-DBY2YGbI",
        "outputId": "7d5e3f80-2322-4417-d41c-f04c7a5c6e5f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training ---\n",
            "Epoch 500/5000, Loss: 1.0967, Accuracy: 0.3417\n",
            "Epoch 1000/5000, Loss: 1.0815, Accuracy: 0.6417\n",
            "Epoch 1500/5000, Loss: 0.9706, Accuracy: 0.8917\n",
            "Epoch 2000/5000, Loss: 0.7176, Accuracy: 0.8500\n",
            "Epoch 2500/5000, Loss: 0.5536, Accuracy: 0.9000\n",
            "Epoch 3000/5000, Loss: 0.4697, Accuracy: 0.9167\n",
            "Epoch 3500/5000, Loss: 0.4164, Accuracy: 0.9250\n",
            "Epoch 4000/5000, Loss: 0.3767, Accuracy: 0.9250\n",
            "Epoch 4500/5000, Loss: 0.3446, Accuracy: 0.9250\n",
            "Epoch 5000/5000, Loss: 0.3177, Accuracy: 0.9417\n",
            "\n",
            "--- After Training ---\n",
            "Loss on Test Set: 0.2991\n",
            "Accuracy on Test Set: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- After Training ---\")\n",
        "predictions_after = model.forward(X_test)\n",
        "loss_after = cross_entropy_loss(Y_test, predictions_after)\n",
        "accuracy_after = accuracy(Y_test, predictions_after)\n",
        "print(f\"Loss on Test Set: {loss_after:.4f}\")\n",
        "print(f\"Accuracy on Test Set: {accuracy_after:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHWkN2ypYP4c",
        "outputId": "22fef7f8-4fe2-484d-f342-b48280c8b055"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- After Training ---\n",
            "Loss on Test Set: 0.2991\n",
            "Accuracy on Test Set: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F6QSyWigZAAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}