{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF37ZZJTA3BZhIEO4CYbww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricSiq/Understanding-DeepLearning/blob/main/Experiment_No7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4ZHNRKm3OwU"
      },
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Install Keras Tuner for hyperparameter tuning\n",
        "!pip install -q keras-tuner\n",
        "\n",
        "# Import Keras Tuner library\n",
        "import keras_tuner as kt\n",
        "\n",
        "# --- Data Download and Preparation ---\n",
        "# This block automates downloading the dataset from Kaggle.\n",
        "# It requires a 'kaggle.json' file with your API credentials.\n",
        "try:\n",
        "    # Attempt to install the Kaggle library\n",
        "    !pip install -q kaggle\n",
        "\n",
        "    # Check if kaggle.json is present\n",
        "    if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "        print(\"Kaggle credentials not found. Please upload your kaggle.json file.\")\n",
        "        from google.colab import files\n",
        "        files.upload() # This will prompt you to upload the file\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !mv kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    # Download the dataset if it's not already downloaded\n",
        "    dataset_name = 'paultimothymooney/chest-xray-pneumonia'\n",
        "    zip_file = 'chest-xray-pneumonia.zip'\n",
        "    dataset_dir = 'chest_xray'\n",
        "\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"Downloading dataset: {dataset_name}...\")\n",
        "        !kaggle datasets download -d {dataset_name}\n",
        "        print(f\"Unzipping {zip_file}...\")\n",
        "        !unzip -q {zip_file}\n",
        "        print(\"Dataset successfully downloaded and unzipped.\")\n",
        "    else:\n",
        "        print(\"Dataset already exists. Skipping download.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during data setup: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Data Generators with Augmentation ---\n",
        "try:\n",
        "    # Define paths to the dataset folders\n",
        "    train_dir = os.path.join(dataset_dir, 'chest_xray', 'train')\n",
        "    val_dir = os.path.join(dataset_dir, 'chest_xray', 'val')\n",
        "    test_dir = os.path.join(dataset_dir, 'chest_xray', 'test')\n",
        "\n",
        "    # Basic error check for folder existence\n",
        "    if not all(os.path.exists(d) for d in [train_dir, val_dir, test_dir]):\n",
        "        raise FileNotFoundError(\"Dataset directories not found. Please check paths.\")\n",
        "\n",
        "    # Data augmentation for the training set to prevent overfitting\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Only rescale validation and test data\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Use flow_from_directory to create generators\n",
        "    IMG_SIZE = 150\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    validation_generator = test_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during data preprocessing: {e}\")"
      ],
      "metadata": {
        "id": "VEdxYuyV3WqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define a Hypermodel Building Function ---\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Builds a CNN model with tunable hyperparameters.\n",
        "\n",
        "    Args:\n",
        "      hp: A KerasTuner HyperParameters instance.\n",
        "\n",
        "    Returns:\n",
        "      A compiled Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Tune the number of convolutional layers\n",
        "    num_conv_layers = hp.Int('num_conv_layers', min_value=2, max_value=4, step=1)\n",
        "\n",
        "    for i in range(num_conv_layers):\n",
        "        # Tune the number of filters in each conv layer\n",
        "        filters = hp.Int(f'filters_{i}', min_value=32, max_value=128, step=32)\n",
        "        model.add(Conv2D(\n",
        "            filters=filters,\n",
        "            kernel_size=(3, 3),\n",
        "            activation='relu',\n",
        "            input_shape=(IMG_SIZE, IMG_SIZE, 3) if i == 0 else None\n",
        "        ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flatten the output for the dense layers\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Tune the number of dense layers\n",
        "    num_dense_layers = hp.Int('num_dense_layers', min_value=1, max_value=2, step=1)\n",
        "    for i in range(num_dense_layers):\n",
        "        # Tune the number of units in each dense layer\n",
        "        units = hp.Int(f'units_{i}', min_value=128, max_value=512, step=128)\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        # Tune the dropout rate\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.6, step=0.1)\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    # Final output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Tune the learning rate for the Adam optimizer\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Instantiate and Run the Tuner ---\n",
        "try:\n",
        "    # Use the Hyperband tuner for efficient search\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective='val_accuracy',\n",
        "        max_epochs=20, # Max number of epochs for a single trial\n",
        "        factor=3,      # Reduction factor for Hyperband\n",
        "        directory='my_dir',\n",
        "        project_name='pneumonia_tuning'\n",
        "    )\n",
        "\n",
        "    print(\"Starting hyperparameter search...\")\n",
        "\n",
        "    # Define an EarlyStopping callback to prevent long training on poor models\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "    tuner.search(\n",
        "        train_generator,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[stop_early],\n",
        "        steps_per_epoch=train_generator.samples // BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    print(\"Hyperparameter search complete.\")\n",
        "\n",
        "    # Get the optimal hyperparameters and the best model\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    print(\"\\n--- Best Hyperparameters Found ---\")\n",
        "    print(f\"Number of Conv Layers: {best_hps.get('num_conv_layers')}\")\n",
        "    print(f\"Number of Dense Layers: {best_hps.get('num_dense_layers')}\")\n",
        "    print(f\"Optimal learning rate: {best_hps.get('learning_rate'):.5f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during hyperparameter tuning: {e}\")"
      ],
      "metadata": {
        "id": "O8QA_Xg73gap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}