{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmB8wQaSMZo47H/F5Zb+Xa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricSiq/DeepLearning/blob/main/ANN_using_XOR_gate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "PPmIAOACvXDc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "This step defines a class NeuralNetwork to encapsulate all the functionality of the artificial neural network.\n",
        "The __init__ method is the constructor, which initializes the network's structure. It takes input_size, hidden_size, output_size, and an optional learning_rate as arguments.\n",
        "\n",
        "        \"\"\"\n",
        "        if not all(isinstance(i, int) for i in [input_size, hidden_size, output_size]):\n",
        "            raise TypeError(\"Input, hidden, and output sizes must be integers.\")\n",
        "        if not isinstance(learning_rate, (int, float)):\n",
        "            raise TypeError(\"Learning rate must be a number.\")\n",
        "        if not (0 < learning_rate <= 1):\n",
        "            raise ValueError(\"Learning rate should be between 0 and 1.\")\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Step 2: Initialize weights and biases randomly\n",
        "        \"\"\"\n",
        "         Initialize weights and biases randomly\n",
        "          Inside the __init__ method, this step initializes the network's weights and biases.\n",
        "          Weights are initialized with random values using np.random.randn, which helps the network to start learning from different points.\n",
        "        \"\"\"\n",
        "        # Weights from input to hidden layer\n",
        "        self.weights_ih = np.random.randn(self.input_size, self.hidden_size)\n",
        "        # Biases for hidden layer\n",
        "        self.bias_h = np.zeros((1, self.hidden_size))\n",
        "\n",
        "        # Weights from hidden to output layer\n",
        "        self.weights_ho = np.random.randn(self.hidden_size, self.output_size)\n",
        "        # Biases for output layer\n",
        "        self.bias_o = np.zeros((1, self.output_size))\n",
        "\n",
        "    # Step 3: Define the activation functions (Sigmoid and its derivative)\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        The sigmoid function is a common choice for activation functions in neural networks, as it squashes the input values into a range between 0 and 1, which is useful for modeling probabilities.\n",
        "        The derivative of the sigmoid function is needed during backpropagation to calculate how much the weights should be adjusted.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # Step 4: Implement the feedforward process\n",
        "    def feedforward(self, inputs):\n",
        "        \"\"\"\n",
        "        The feedforward method is responsible for moving data from the input layer, through the hidden layer, to the output layer.\n",
        "        It takes an array of inputs and performs matrix multiplications and additions with the weights and biases.\n",
        "        The sigmoid function is applied at each layer to introduce non-linearity.\n",
        "        \"\"\"\n",
        "        if inputs.shape[1] != self.input_size:\n",
        "            raise ValueError(f\"Input shape mismatch. Expected {self.input_size} features, got {inputs.shape[1]}.\")\n",
        "\n",
        "        # Calculate hidden layer output\n",
        "        self.hidden_layer_input = np.dot(inputs, self.weights_ih) + self.bias_h\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Calculate final output\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_ho) + self.bias_o\n",
        "        self.output = self.sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    # Step 5: Implement the backpropagation process\n",
        "    def backpropagation(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        The backpropagation method is the core of the learning algorithm. After a feedforward pass, it calculates the error between the network's output and the desired targets.\n",
        "        It then propagates this error backward through the network to determine how much each weight and bias contributed to the error.\n",
        "        This information is used to update the weights and biases using the calculated deltas and the learning_rate, aiming to reduce the overall error in the next iteration.\n",
        "        \"\"\"\n",
        "        if targets.shape != self.output.shape:\n",
        "            raise ValueError(f\"Target shape mismatch. Expected {self.output.shape}, got {targets.shape}.\")\n",
        "\n",
        "        # Calculate output layer error\n",
        "        output_error = targets - self.output\n",
        "        output_delta = output_error * self.sigmoid_derivative(self.output)\n",
        "\n",
        "        # Calculate hidden layer error\n",
        "        hidden_error = output_delta.dot(self.weights_ho.T)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Update weights and biases for output layer\n",
        "        self.weights_ho += self.hidden_layer_output.T.dot(output_delta) * self.learning_rate\n",
        "        self.bias_o += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "        # Update weights and biases for hidden layer\n",
        "        self.weights_ih += inputs.T.dot(hidden_delta) * self.learning_rate\n",
        "        self.bias_h += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    # Step 6: Define the training loop\n",
        "    def train(self, inputs, targets, epochs):\n",
        "        \"\"\"\n",
        "        The train method takes the input data, target data, and the number of epochs (iterations) as arguments.\n",
        "        It repeatedly calls the feedforward and backpropagation methods for the specified number of epochs.\n",
        "        A check is included to print the mean absolute error every 1000 epochs, allowing for monitoring of the training progress.\n",
        "        \"\"\"\n",
        "        if not isinstance(epochs, int) or epochs <= 0:\n",
        "            raise ValueError(\"Epochs must be a positive integer.\")\n",
        "        if not isinstance(inputs, np.ndarray) or not isinstance(targets, np.ndarray):\n",
        "            raise TypeError(\"Inputs and targets must be numpy arrays.\")\n",
        "        if inputs.shape[0] != targets.shape[0]:\n",
        "            raise ValueError(\"Inputs and targets must have the same number of rows.\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Feedforward and backpropagation for each data point\n",
        "            self.feedforward(inputs)\n",
        "            self.backpropagation(inputs, targets)\n",
        "\n",
        "            # Print error every 1000 epochs to monitor progress\n",
        "            if (epoch + 1) % 1000 == 0:\n",
        "                error = np.mean(np.abs(targets - self.output))\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Error: {error:.4f}\")\n",
        "\n",
        "    # Step 7: Define a method for making predictions\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"\n",
        "        The predict method is a simple utility function that uses the trained network to make a prediction.\n",
        "        It takes new input data, performs a feedforward pass, and then rounds the final output to the nearest integer (0 or 1), which is suitable for the binary classification problem of the XOR gate.\n",
        "        \"\"\"\n",
        "        output = self.feedforward(inputs)\n",
        "        return np.round(output)"
      ],
      "metadata": {
        "id": "SlyZoaDpul5R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvGcwrF0tvrc",
        "outputId": "ae7aa3ba-c841-48ca-80ff-38abfbd87339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the ANN for XOR gate with 10000 epochs...\n",
            "Epoch 1000/10000, Error: 0.0913\n",
            "Epoch 2000/10000, Error: 0.0473\n",
            "Epoch 3000/10000, Error: 0.0351\n",
            "Epoch 4000/10000, Error: 0.0290\n",
            "Epoch 5000/10000, Error: 0.0251\n",
            "Epoch 6000/10000, Error: 0.0225\n",
            "Epoch 7000/10000, Error: 0.0205\n",
            "Epoch 8000/10000, Error: 0.0189\n",
            "Epoch 9000/10000, Error: 0.0177\n",
            "Epoch 10000/10000, Error: 0.0166\n",
            "Training complete.\n",
            "\n",
            "Testing the trained network:\n",
            "Input: [[0 0]], Predicted: 0.0, Actual: 0\n",
            "Input: [[0 1]], Predicted: 1.0, Actual: 1\n",
            "Input: [[1 0]], Predicted: 1.0, Actual: 1\n",
            "Input: [[1 1]], Predicted: 0.0, Actual: 0\n",
            "\n",
            "All predictions are correct! The ANN successfully learned the XOR gate.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 8: Prepare the XOR dataset\n",
        "\"\"\"\n",
        "It defines the training data for the XOR problem.\n",
        " X contains the four possible binary inputs [0, 0], [0, 1], [1, 0], and [1, 1].\n",
        " y contains the corresponding XOR outputs [0], [1], [1], and [0].\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    # XOR inputs\n",
        "    X = np.array([\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ])\n",
        "\n",
        "    # XOR outputs\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    # Step 9: Initialize and train the neural network\n",
        "    # Network with 2 inputs, 4 hidden neurons, and 1 output\n",
        "    \"\"\"\n",
        "    It then calls the train method with the prepared XOR data and a specified number of epochs (10,000 in this case) to train the network.\n",
        "    A try...except block is used to catch and handle any TypeError or ValueError that might occur during setup or training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ann = NeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
        "        epochs = 10000\n",
        "        print(f\"Training the ANN for XOR gate with {epochs} epochs...\")\n",
        "        ann.train(X, y, epochs)\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "        # Step 10: Test the trained network\n",
        "        \"\"\"\n",
        "        After training is complete, this final step tests the network's ability to solve the XOR problem.\n",
        "        It loops through each of the four XOR inputs, uses the predict method to get an output, and then compares the predicted output with the actual output.\n",
        "        An assert statement is used to programmatically verify that the network's predictions are correct for all test cases.\n",
        "        \"\"\"\n",
        "        print(\"\\nTesting the trained network:\")\n",
        "        for i in range(len(X)):\n",
        "            input_data = X[i:i+1]\n",
        "            predicted_output = ann.predict(input_data)\n",
        "            actual_output = y[i]\n",
        "            print(f\"Input: {input_data}, Predicted: {predicted_output[0][0]}, Actual: {actual_output[0]}\")\n",
        "            assert predicted_output[0][0] == actual_output[0], \"Prediction failed for a test case.\"\n",
        "\n",
        "        print(\"\\nAll predictions are correct! The ANN successfully learned the XOR gate.\")\n",
        "\n",
        "    except (TypeError, ValueError) as e:\n",
        "        print(f\"An error occurred during network setup or training: {e}\")"
      ]
    }
  ]
}