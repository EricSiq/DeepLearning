{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY9f/KS2x7pzkHAxnoepQH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricSiq/Understanding-DeepLearning/blob/main/Experiment_No6_XRay_Classification_Pneumonia_using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO8uxb71JZ8u"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to the dataset\n",
        "train_dir = '/content/chest_xray/train'\n",
        "val_dir = '/content/chest_xray/val'\n",
        "test_dir = '/content/chest_xray/test'\n",
        "\n",
        "# Use ImageDataGenerator for data preprocessing and augmentation\n",
        "# Rescale pixel values to the range [0, 1]\n",
        "# Apply data augmentation to the training set to prevent overfitting\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Only rescale the validation and test sets (no augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create data generators from directories\n",
        "# This automatically infers labels from subfolder names\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),  # Resize all images to 150x150 pixels\n",
        "    batch_size=32,\n",
        "    class_mode='binary'      # 'binary' for our two classes (normal/pneumonia)\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False             # Important to set shuffle=False for evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "DgOuzKtpJl6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a series of Convolutional and Pooling layers\n",
        "# These layers automatically learn features from the images\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flatten the output to feed into the dense layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dropout layer to reduce overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add a dense layer with ReLU activation\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "# Final output layer with a sigmoid activation for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "# Use Adam optimizer and binary crossentropy for loss\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yhgclSEDJqfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the fit method with generators\n",
        "# We train for 10 epochs, a good starting point\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('pneumonia_classifier.h5')"
      ],
      "metadata": {
        "id": "SAF_CLbCJsFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the unseen test data\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# You can also visualize the training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pNEo_1abJtcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}