{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricSiq/Understanding-DeepLearning/blob/main/xp7_ChestXray_PneumoniaDetection_using_CNNs_withHyperparameterTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import core libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Install Keras Tuner for hyperparameter tuning\n",
        "!pip install -q keras-tuner\n",
        "\n",
        "# Import Keras Tuner library\n",
        "import keras_tuner as kt\n",
        "\n",
        "# --- Data Download and Preparation ---\n",
        "# This block automates downloading the dataset from Kaggle.\n",
        "# It requires a 'kaggle.json' file with your API credentials.\n",
        "try:\n",
        "    # Attempt to install the Kaggle library\n",
        "    !pip install -q kaggle\n",
        "\n",
        "    # Check if kaggle.json is present\n",
        "    if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "        print(\"Kaggle credentials not found. Please upload your kaggle.json file.\")\n",
        "        from google.colab import files\n",
        "        files.upload() # This will prompt you to upload the file\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !mv kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    # Download the dataset if it's not already downloaded\n",
        "    dataset_name = 'paultimothymooney/chest-xray-pneumonia'\n",
        "    zip_file = 'chest-xray-pneumonia.zip'\n",
        "    dataset_dir = 'chest_xray'\n",
        "\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"Downloading dataset: {dataset_name}...\")\n",
        "        !kaggle datasets download -d {dataset_name}\n",
        "        print(f\"Unzipping {zip_file}...\")\n",
        "        !unzip -q {zip_file}\n",
        "        print(\"Dataset successfully downloaded and unzipped.\")\n",
        "    else:\n",
        "        print(\"Dataset already exists. Skipping download.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during data setup: {e}\")\n",
        "\n",
        "# --- Define Data Generators with Augmentation ---\n",
        "try:\n",
        "    # Define paths to the dataset folders\n",
        "    train_dir = os.path.join(dataset_dir, 'chest_xray', 'train')\n",
        "    val_dir = os.path.join(dataset_dir, 'chest_xray', 'val')\n",
        "    test_dir = os.path.join(dataset_dir, 'chest_xray', 'test')\n",
        "\n",
        "    # Basic error check for folder existence\n",
        "    if not all(os.path.exists(d) for d in [train_dir, val_dir, test_dir]):\n",
        "        raise FileNotFoundError(\"Dataset directories not found. Please check paths.\")\n",
        "\n",
        "    # Data augmentation for the training set to prevent overfitting\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=15, # Reduced range for faster augmentation\n",
        "        zoom_range=0.1,    # Reduced range\n",
        "        horizontal_flip=True,\n",
        "    )\n",
        "\n",
        "    # Only rescale validation and test data\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Use flow_from_directory to create generators\n",
        "    IMG_SIZE = 150\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    validation_generator = test_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during data preprocessing: {e}\")\n",
        "\n",
        "\n",
        "# --- Define a Hypermodel Building Function with a smaller search space ---\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Builds a CNN model with a simplified, faster-to-tune hyperparameter search space.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # We will only tune the number of filters and a single dense layer's units\n",
        "\n",
        "    # Tunable number of filters for the first conv layer\n",
        "    filters_1 = hp.Choice('filters_1', values=[32, 64])\n",
        "    model.add(Conv2D(\n",
        "        filters=filters_1,\n",
        "        kernel_size=(3, 3),\n",
        "        activation='relu',\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        "    ))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Tunable number of filters for the second conv layer\n",
        "    filters_2 = hp.Choice('filters_2', values=[64, 128])\n",
        "    model.add(Conv2D(\n",
        "        filters=filters_2,\n",
        "        kernel_size=(3, 3),\n",
        "        activation='relu',\n",
        "    ))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Tunable number of units for a single dense layer\n",
        "    units = hp.Choice('units', values=[256, 512])\n",
        "    model.add(Dense(units=units, activation='relu'))\n",
        "\n",
        "    # Tunable dropout rate\n",
        "    dropout_rate = hp.Choice('dropout', values=[0.3, 0.5])\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    # Final output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Tunable learning rate\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-4, 1e-3])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Instantiate and Run the Tuner with a small number of trials ---\n",
        "try:\n",
        "    tuner = kt.RandomSearch(\n",
        "        build_model,\n",
        "        objective='val_accuracy',\n",
        "        max_trials=5, # Limit to only 5 unique models\n",
        "        executions_per_trial=1, # No need to run the same model multiple times\n",
        "        directory='my_dir_simple',\n",
        "        project_name='pneumonia_simple_tuning'\n",
        "    )\n",
        "\n",
        "    print(\"Starting hyperparameter search...\")\n",
        "\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "    tuner.search(\n",
        "        train_generator,\n",
        "        epochs=5, # Significantly reduced number of epochs per trial\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[stop_early],\n",
        "        steps_per_epoch=train_generator.samples // BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    print(\"Hyperparameter search complete.\")\n",
        "\n",
        "    # Get the optimal hyperparameters and the best model\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    print(\"\\n--- Best Hyperparameters Found ---\")\n",
        "    print(f\"Optimal filters_1: {best_hps.get('filters_1')}\")\n",
        "    print(f\"Optimal filters_2: {best_hps.get('filters_2')}\")\n",
        "    print(f\"Optimal units: {best_hps.get('units')}\")\n",
        "    print(f\"Optimal learning rate: {best_hps.get('learning_rate'):.5f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during hyperparameter tuning: {e}\")"
      ],
      "metadata": {
        "id": "K0rC3mAaXgv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeXQw7rLXhIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}